{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp - install nltk (natural language tool kit)\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package nltk:\n",
      "\n",
      "NAME\n",
      "    nltk\n",
      "\n",
      "DESCRIPTION\n",
      "    The Natural Language Toolkit (NLTK) is an open source Python library\n",
      "    for Natural Language Processing.  A free online book is available.\n",
      "    (If you use the library for academic research, please cite the book.)\n",
      "    \n",
      "    Steven Bird, Ewan Klein, and Edward Loper (2009).\n",
      "    Natural Language Processing with Python.  O'Reilly Media Inc.\n",
      "    https://www.nltk.org/book/\n",
      "    \n",
      "    isort:skip_file\n",
      "    \n",
      "    @version: 3.9.1\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    app (package)\n",
      "    book\n",
      "    ccg (package)\n",
      "    chat (package)\n",
      "    chunk (package)\n",
      "    classify (package)\n",
      "    cli\n",
      "    cluster (package)\n",
      "    collections\n",
      "    collocations\n",
      "    compat\n",
      "    corpus (package)\n",
      "    data\n",
      "    decorators\n",
      "    downloader\n",
      "    draw (package)\n",
      "    featstruct\n",
      "    grammar\n",
      "    help\n",
      "    inference (package)\n",
      "    internals\n",
      "    jsontags\n",
      "    langnames\n",
      "    lazyimport\n",
      "    lm (package)\n",
      "    metrics (package)\n",
      "    misc (package)\n",
      "    parse (package)\n",
      "    probability\n",
      "    sem (package)\n",
      "    sentiment (package)\n",
      "    stem (package)\n",
      "    tabdata\n",
      "    tag (package)\n",
      "    tbl (package)\n",
      "    test (package)\n",
      "    text\n",
      "    tgrep\n",
      "    tokenize (package)\n",
      "    toolbox\n",
      "    translate (package)\n",
      "    tree (package)\n",
      "    treeprettyprinter\n",
      "    treetransforms\n",
      "    twitter (package)\n",
      "    util\n",
      "    wsd\n",
      "\n",
      "SUBMODULES\n",
      "    agreement\n",
      "    aline\n",
      "    api\n",
      "    arlstem\n",
      "    arlstem2\n",
      "    association\n",
      "    bleu_score\n",
      "    bllip\n",
      "    boxer\n",
      "    brill\n",
      "    brill_trainer\n",
      "    casual\n",
      "    chart\n",
      "    chrf_score\n",
      "    cistem\n",
      "    confusionmatrix\n",
      "    corenlp\n",
      "    crf\n",
      "    decisiontree\n",
      "    dependencygraph\n",
      "    destructive\n",
      "    discourse\n",
      "    distance\n",
      "    drt\n",
      "    earleychart\n",
      "    evaluate\n",
      "    featurechart\n",
      "    gale_church\n",
      "    gdfa\n",
      "    gleu_score\n",
      "    glue\n",
      "    hmm\n",
      "    hunpos\n",
      "    ibm1\n",
      "    ibm2\n",
      "    ibm3\n",
      "    ibm4\n",
      "    ibm5\n",
      "    ibm_model\n",
      "    isri\n",
      "    lancaster\n",
      "    legality_principle\n",
      "    lfg\n",
      "    linearlogic\n",
      "    logic\n",
      "    mace\n",
      "    malt\n",
      "    mapping\n",
      "    maxent\n",
      "    megam\n",
      "    meteor_score\n",
      "    mwe\n",
      "    naivebayes\n",
      "    named_entity\n",
      "    nist_score\n",
      "    nonprojectivedependencyparser\n",
      "    paice\n",
      "    pchart\n",
      "    perceptron\n",
      "    phrase_based\n",
      "    porter\n",
      "    positivenaivebayes\n",
      "    projectivedependencyparser\n",
      "    prover9\n",
      "    punkt\n",
      "    recursivedescent\n",
      "    regexp\n",
      "    relextract\n",
      "    repp\n",
      "    resolution\n",
      "    ribes_score\n",
      "    rslp\n",
      "    rte_classify\n",
      "    scikitlearn\n",
      "    scores\n",
      "    segmentation\n",
      "    senna\n",
      "    sequential\n",
      "    sexpr\n",
      "    shiftreduce\n",
      "    simple\n",
      "    snowball\n",
      "    sonority_sequencing\n",
      "    spearman\n",
      "    stack_decoder\n",
      "    stanford\n",
      "    stanford_segmenter\n",
      "    tableau\n",
      "    tadm\n",
      "    textcat\n",
      "    texttiling\n",
      "    tnt\n",
      "    toktok\n",
      "    transitionparser\n",
      "    treebank\n",
      "    viterbi\n",
      "    weka\n",
      "    wordnet\n",
      "\n",
      "FUNCTIONS\n",
      "    demo()\n",
      "        # FIXME:  override any accidentally imported demo, see https://github.com/nltk/nltk/issues/2116\n",
      "    \n",
      "    tee(iterable, n=2, /)\n",
      "        Returns a tuple of n independent iterators.\n",
      "\n",
      "DATA\n",
      "    PRETRAINED_TAGGERS = {'eng': 'taggers/averaged_perceptron_tagger_eng/'...\n",
      "    SLASH = *slash*\n",
      "    TYPE = *type*\n",
      "    __author_email__ = 'nltk.team@gmail.com'\n",
      "    __classifiers__ = ['Development Status :: 5 - Production/Stable', 'Int...\n",
      "    __copyright__ = 'Copyright (C) 2001-2024 NLTK Project.\\n\\nDistribut......\n",
      "    __keywords__ = ['NLP', 'CL', 'natural language processing', 'computati...\n",
      "    __license__ = 'Apache License, Version 2.0'\n",
      "    __longdescr__ = 'The Natural Language Toolkit (NLTK) is a Python ...LT...\n",
      "    __maintainer__ = 'NLTK Team'\n",
      "    __maintainer_email__ = 'nltk.team@gmail.com'\n",
      "    __url__ = 'https://www.nltk.org/'\n",
      "    app = <LazyModule 'nltk.app'>\n",
      "    chat = <LazyModule 'nltk.chat'>\n",
      "    corpus = <LazyModule 'nltk.corpus'>\n",
      "    infile = <_io.TextIOWrapper name='c:\\\\Users\\\\DELL\\\\AppDat...kages\\\\nlt...\n",
      "    json_tags = {'!nltk.tag.BrillTagger': <class 'nltk.tag.brill.BrillTagg...\n",
      "    toolbox = <LazyModule 'nltk.toolbox'>\n",
      "    version_file = r'c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python38\\...\n",
      "\n",
      "VERSION\n",
      "    3.9.1\n",
      "\n",
      "AUTHOR\n",
      "    NLTK Team\n",
      "\n",
      "FILE\n",
      "    c:\\users\\dell\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "#stop words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop=stopwords.words('english')\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop) #total 198 stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Iam',\n",
       " 'Nandana',\n",
       " ',',\n",
       " 'a',\n",
       " 'computer',\n",
       " 'Science',\n",
       " 'student',\n",
       " 'from',\n",
       " 'Cochin',\n",
       " 'University',\n",
       " 'in',\n",
       " 'Kochi']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenization\n",
    "\n",
    "sentence='Iam Nandana,a computer Science student from Cochin University in Kochi'\n",
    "from nltk.tokenize import word_tokenize\n",
    "words=word_tokenize(sentence)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<nltk.tokenize.casual.TweetTokenizer object at 0x000001EB0F990340>\n"
     ]
    }
   ],
   "source": [
    "#tweetTokenizer\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "w=TweetTokenizer(sentence)\n",
    "print(w)  #output is an object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural',\n",
       " 'language',\n",
       " 'toolkit',\n",
       " '(',\n",
       " 'nltk',\n",
       " ')',\n",
       " 'open',\n",
       " 'source',\n",
       " 'python',\n",
       " 'library',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing stop words\n",
    "\n",
    "sent1='''The Natural Language Toolkit (NLTK) is an open source Python library\n",
    "    for Natural Language Processing.''' \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stword=stopwords.words('english')\n",
    "words1=word_tokenize(sent1)\n",
    "res=list(i.lower() for i in words1 if i.lower() not in stword)\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The', 'Natural')\n",
      "('Natural', 'Language')\n",
      "('Language', 'Toolkit')\n",
      "('Toolkit', '(')\n",
      "('(', 'NLTK')\n",
      "('NLTK', ')')\n",
      "(')', 'is')\n",
      "('is', 'an')\n",
      "('an', 'open')\n",
      "('open', 'source')\n",
      "('source', 'Python')\n",
      "('Python', 'library')\n",
      "('library', 'for')\n",
      "('for', 'Natural')\n",
      "('Natural', 'Language')\n",
      "('Language', 'Processing')\n",
      "('Processing', '.')\n"
     ]
    }
   ],
   "source": [
    "#n_gram\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "gram=ngrams(sequence=nltk.word_tokenize(sent1),n=2) #2gram\n",
    "for i in gram:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "programming  :  program\n",
      "jumping  :  jump\n",
      "bought  :  bought\n",
      "thought  :  thought\n",
      "wrote  :  wrote\n",
      "hint  :  hint\n",
      "hated  :  hate\n",
      "roams  :  roam\n",
      "reached  :  reach\n"
     ]
    }
   ],
   "source": [
    "#stemming\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "words=['programming','jumping','bought','thought','wrote','hint','hated','roams','reached']\n",
    "for i in words:\n",
    "    print(i, \" : \",ps.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "programming  :  program\n",
      "jumping  :  jump\n",
      "bought  :  bought\n",
      "thought  :  thought\n",
      "wrote  :  wrote\n",
      "hint  :  hint\n",
      "hated  :  hate\n",
      "roams  :  roam\n",
      "reached  :  reach\n"
     ]
    }
   ],
   "source": [
    "#snow ball stemmer\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "st=SnowballStemmer('english')\n",
    "for i in words:\n",
    "    print(i, \" : \",st.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "programs : program\n",
      "eating : eating\n",
      "reaches : reach\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization\n",
    "#lemmatization mostly works when words end with 's'\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lem=WordNetLemmatizer()\n",
    "print('rocks',':',lem.lemmatize('rocks'))\n",
    "print('programs',':',lem.lemmatize('programs'))\n",
    "print('eating',':',lem.lemmatize('eating'))\n",
    "print('reaches',':',lem.lemmatize('reaches'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wonderful   peacock123     '"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove special characters\n",
    "#re - regular expression - used for pattern matching\n",
    "#patterns - [A-Z] [a-z] [0-9]\n",
    "\n",
    "import re\n",
    "str1='Wonderful_ @peacock123#)!!^'\n",
    "str2=re.sub('[^A-Za-z0-9]',' ',str1) #substitute  #^ acts as not here - everything except\n",
    "str2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
